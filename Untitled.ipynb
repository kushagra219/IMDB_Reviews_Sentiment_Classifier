{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 500\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('IMDB Dataset.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = reviews['review']\n",
    "labels = reviews['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = []\n",
    "for s in sentences:\n",
    "    word_tokens = word_tokenize(s) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentences.append(\" \".join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1        positive\n",
       "2        positive\n",
       "3        negative\n",
       "4        positive\n",
       "           ...   \n",
       "49995    positive\n",
       "49996    negative\n",
       "49997    negative\n",
       "49998    negative\n",
       "49999    negative\n",
       "Name: sentiment, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(word):\n",
    "    if word == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "labels = labels.apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17510\n",
       "1    17490\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "[[ 28   5   2 ...   0   0   0]\n",
      " [  4 388 119 ...   0   0   0]\n",
      " [ 11 190  12 ...   0   0   0]\n",
      " ...\n",
      " [ 10 811  21 ...   0   0   0]\n",
      " [  1 125  10 ...   0   0   0]\n",
      " [ 12  18  91 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sequences))\n",
    "print(training_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17510\n"
     ]
    }
   ],
   "source": [
    "print(len(training_labels[training_labels==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.4860 - acc: 0.7806 - val_loss: 0.3185 - val_acc: 0.8733\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.2624 - acc: 0.8986 - val_loss: 0.2661 - val_acc: 0.8980\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.2159 - acc: 0.9190 - val_loss: 0.2604 - val_acc: 0.8968\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.1901 - acc: 0.9286 - val_loss: 0.2616 - val_acc: 0.8999\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.1714 - acc: 0.9376 - val_loss: 0.2760 - val_acc: 0.8945\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.1590 - acc: 0.9412 - val_loss: 0.2751 - val_acc: 0.8970\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.1452 - acc: 0.9478 - val_loss: 0.2906 - val_acc: 0.8927\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.1351 - acc: 0.9523 - val_loss: 0.3038 - val_acc: 0.8917\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.1263 - acc: 0.9563 - val_loss: 0.3302 - val_acc: 0.8857\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.1173 - acc: 0.9605 - val_loss: 0.3330 - val_acc: 0.8881\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    sentence = [sentence]\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    sentiment = model.predict(padded)\n",
    "    return (\"positive\",float(sentiment)) if sentiment > 0.5 else (\"negative\", float(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ON DARK SERIES REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 0.017044639214873314)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"\"\"While Season One was OK, Season Two proved the law of diminishing returns with an increasingly \n",
    "convoluted plot and no significant new ideas. Season Three has become a dull, hyper-complex mess, where the characters \n",
    "mostly stare a) open mouthed or b) teary eyed (sometimes both!) at some 'revelation' or other in lieu of an actual plot \n",
    "or anything even remotely exciting or interesting. To distract from this narrative vacuum, a parallel world and even more \n",
    "time travel destinations, more characters, older/younger selves and parallel selves is introduced with the effect of \n",
    "creating an utterly boring and confusing shambles. The actors aren't given a chance to show their skill as they are only \n",
    "allowed to either utter some semi philosophical nonsense or revert to expressions a) and b) above. Sadly Ben Frost's \n",
    "soundtrack and the D.O.P. are wasted on this.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 0.06525209546089172)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"\"\"The show wants to be a serious mystery. It want's to be grim. There is a constant synth sound in \n",
    "nearly every scene foreshadowing grim things are coming,even on the most banal scenes.It feels like someone in post \n",
    "production wanted to play around with Dolby Atmos as much as possible instead of doing a proper sound mix that supports \n",
    "the action on screen. From a technical aspect the sound is well engineered, but it doesn't do anything for the show.\n",
    "And you get a perfect exercise in overacting by the semi talented cast. Every line is delivered frowned with meaning, \n",
    "swollen as if there were an underlying meaning to every word, glimpse or motion.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 0.999691367149353)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"\"\"Netflix list this as a \"teen\" series. I think any discerning watcher of any age will get a lot\n",
    "out of this dark, compelling mystery. Set in a small German town where weird things are happening, it's a mind bending,\n",
    "time shifting saga with each character given weight and importance. The acting is first rate, especially the young actors \n",
    "portraying the kids. This is not \"Stranger Things\" Euro style - its nothing like it. Eerie and intelligent, you have to \n",
    "really concentrate on the narrative to get all the nuances of the plot and characters. Very, very good. A solid 8 from me\n",
    "and I'm really harsh with my ratings! I'm two episodes into Season 2, and it's as good as Season 1, so far. More mysteries\n",
    "unfolding and characters you really care about. Recommended.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f66499eb5b0e940f04492bf0ec819b229fb8ec4462842231be199e68991f4583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
